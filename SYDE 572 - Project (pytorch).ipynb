{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Project: Household Item Classifier\n",
    "\n",
    "**Author**:      Benjamin Luo \\\n",
    "**Email**: b33luo@uwaterloo.ca \\\n",
    "**Course**: SYDE 572 (F23)\n",
    "\n",
    "**Python ver**: 3.11.6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './train2'\n",
    "test_dir = './test2'\n",
    "validation_dir = './validation'\n",
    "\n",
    "batch_size = 16\n",
    "lr = 0.12\n",
    "max_epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch==2.1.1\n",
    "# %pip install torchvision==0.16.1\n",
    "# %pip install matplotlib\n",
    "# %pip install scikit-learn\n",
    "# %pip install pandas\n",
    "# %pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")  # For mac users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(255),  # Resize to fit the model input size\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),  # Flip because half the images are inverted\n",
    "    # transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  # Fine-tuning\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(255),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "trainset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size)\n",
    "\n",
    "classes = trainset.classes  # For mapping ImageFolder indices to class names\n",
    "\n",
    "torch.manual_seed(1217)\n",
    "torch.mps.manual_seed(1217)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(1217)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "Here, ResNet50 and MobileNetv2 were shown to have the greatest results. ResNet50 was more consistent so it is used for the final submission\n",
    "\n",
    "For the optimizer, ADAM generally had noticibly better accuracy than SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Loading the pretrained model\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "# model = models.mobilenet_v2(pretrained=True)\n",
    "model.to(device=device)\n",
    "\n",
    "# Freezing the weights\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "# Tailoring the final model to output 22 classes (corresponding to 22 household items)\n",
    "model.fc = nn.Linear(model.fc.in_features, 22, device=device)  #resnet50\n",
    "# model.classifier[1] = nn.Linear(model.last_channel, 22, device=device)  #mobilenetv2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = model.parameters()\n",
    "\n",
    "optimizer = optim.SGD(params, lr=lr)\n",
    "# optimizer = optim.Adam(params, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,trainloader,optimizer,criterion):\n",
    "  model.train()\n",
    "\n",
    "  total_loss = []\n",
    "  for data in trainloader:\n",
    "    images, labels = data\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward() \n",
    "    total_loss.append(loss.item())\n",
    "    optimizer.step()\n",
    "  return np.average(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_train(model, testloader, testset):\n",
    "  model.eval()\n",
    "  correct = 0.0\n",
    "\n",
    "  for data in testloader:\n",
    "    images, labels = data\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = model(images)\n",
    "    _,preds = outputs.max(1)\n",
    "    correct += preds.eq(labels).sum()\n",
    "\n",
    "  return correct.float()/len(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code trains the model using the trainloader data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "  train(model, trainloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and then returns the accuracy of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9342359900474548\n"
     ]
    }
   ],
   "source": [
    "accu = eval_train(model, testloader, testset)\n",
    "print('Accuracy: ', accu.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix 1: Outputting Results in a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a custom class based on the ImageFolder class. This is important for extracting the pathname from the file for outputting into the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "   def __getitem__(self, index):\n",
    "       img, label = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "       path = self.imgs[index][0]\n",
    "       return (img, label, path)\n",
    "    \n",
    "validationset = ImageFolderWithPaths(validation_dir, transform=test_transform)\n",
    "validationloader = torch.utils.data.DataLoader(validationset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the model on the entire validation set and keep track of the predicted labels and reformat the dataframe before saving it into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_pred(model):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    filenames = []\n",
    "    with torch.no_grad():\n",
    "        for images, _, paths in validationloader:\n",
    "            images = images.to(device)\n",
    "            output = model(images)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            filenames.extend(paths)\n",
    "\n",
    "    return predictions, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predictions, filenames = validation_pred(model)\n",
    "predictions = [classes[predict] for predict in predictions]\n",
    "filenames = [int(filename.rsplit('/', 1)[1][:-4]) for filename in filenames]\n",
    "\n",
    "# Create a DataFrame from the predictions\n",
    "df = pd.DataFrame(predictions, columns=['category'])\n",
    "df.insert(0, 'id', filenames)\n",
    "df = df.sort_values(by=df.columns[0])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('luo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix 2: Visualizing Outputs\n",
    "\n",
    "The below cell is experimental code that randomly selects an image from the test set and plots it via matplotlib, as well as prints its true and predicted label for empirical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import random\n",
    "\n",
    "# def rand_pred(model):\n",
    "#     model.eval()\n",
    "#     image, label = random.choice(testloader.dataset)\n",
    "#     mps_image = image.to(device)\n",
    "\n",
    "#     outputs = model(mps_image.unsqueeze(0))\n",
    "#     _,pred = outputs.max(1)\n",
    "    \n",
    "#     print(\"Predicted:\", classes[pred.item()])\n",
    "#     print(\"True:\", classes[label])\n",
    "#     plt.imshow(image.permute(1, 2, 0))\n",
    "\n",
    "# rand_pred(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix 3: Hyperparameter Optimization\n",
    "\n",
    "Grid search is manually implemented. The hyperparameters of interest are **epochs, learning rate (lr), and batch size**. Approximately 600 combinations are attempted in total for the following pretrained models:\n",
    "\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* ResNet34\n",
    "* MobileNetv2 (regular and quantized)\n",
    "* MobileNetv3 (small and large)\n",
    "\n",
    "For the optimizer, both SGD and ADAM were attempted but ADAM consistently returned better results and was subsequently used for more trials. The grid parameters for ADAM are:\n",
    "\n",
    "```py\n",
    "    'max_epochs': [8, 10, 12, 14, 16, 18, 20]  # Must be kept small for transfer learning\n",
    "    'lr': [0.005, 0.01, 0.015]                 # Optimal lr range for ADAM optimizer\n",
    "    'batch_size': [8, 14, 16, 24, 36, 48, 64]  # Trying different values (for fine-tuning)\n",
    "```\n",
    "\n",
    "MobileNetv2 and ResNet50 produced the best results, with ResNet50 being more consistent and the selected algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# grid_params = {\n",
    "#     'max_epochs': [13],\n",
    "#     'lr': [0.12],\n",
    "#     'batch_size': [8, 16, 22, 24, 32, 36, 48, 52, 56, 64]\n",
    "# }\n",
    "\n",
    "# import torchvision.models as models\n",
    "\n",
    "# for epochs in grid_params['max_epochs']:\n",
    "#     for lrs in grid_params['lr']:\n",
    "#         for batch_sizes in grid_params['batch_size']:\n",
    "\n",
    "#             seed = random.randint(0, 10000)\n",
    "#             torch.manual_seed(seed)\n",
    "#             torch.mps.manual_seed(seed)\n",
    "#             torch.backends.cudnn.deterministic = True\n",
    "#             random.seed(seed)\n",
    "#             np.random.seed(seed)\n",
    "\n",
    "#             # Load data\n",
    "#             trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_sizes, shuffle=True)\n",
    "#             testloader = torch.utils.data.DataLoader(testset, batch_size=batch_sizes)\n",
    "\n",
    "#             # Load model\n",
    "#             model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "#             # model = models.mobilenet_v2(pretrained=True)\n",
    "#             # models.quantization.mobilenet_v2(pretrained='IMAGENET1K_QNNPACK_V1')\n",
    "\n",
    "#             # model = models.mobilenet_v3_large(pretrained=True)\n",
    "#             # model = models.mobilenet_v3_small(pretrained=True)\n",
    "#             # model = models.vgg19()\n",
    "#             model.to(device=device)\n",
    "\n",
    "#             # Freeze model\n",
    "#             for param in model.parameters():\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "#             # Define final layer resnet50\n",
    "#             model.fc = nn.Linear(model.fc.in_features, 22, device=device)\n",
    "#             # model = nn.Sequential(model, nn.Dropout(0.5))\n",
    "#             # model.classifier[1] = nn.Linear(model.last_channel, 22, device=device)  #mobilenetv2\n",
    "#             # model.classifier = nn.Linear(960, 22, device=device)  #mobilenet v3 large\n",
    "#             # model.classifier[-1] = nn.Linear(1024, 22, device=device)  #mobilenet v3 small\n",
    "#             # model.classifier[-1] = torch.nn.Linear(model.classifier[-1].in_features, 22, device=device)  #vgg19\n",
    "\n",
    "#             criterion = nn.CrossEntropyLoss()\n",
    "#             params = model.parameters()\n",
    "\n",
    "#             optimizer = optim.SGD(params, lr=lrs)\n",
    "#             # optimizer = optim.Adam(params, lr=lrs)\n",
    "\n",
    "#             # Training\n",
    "#             for ep in range(epochs):\n",
    "#                 train(model, trainloader, optimizer, criterion)\n",
    "            \n",
    "#             # Test accuracy and output\n",
    "#             accu = eval_train(model, testloader, testset)\n",
    "#             print ('Accuracy: {}, epoch: {}, lr: {}, batch size: {}, seed: {}'.format(accu.item(), epochs, lrs, batch_sizes, seed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
